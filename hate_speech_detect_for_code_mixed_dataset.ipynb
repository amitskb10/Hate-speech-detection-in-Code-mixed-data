{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpbDi8E6vBgwbqpgEm3N9p"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmzwYAhtwbDV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Try to import TensorFlow (CNN/LSTM & contrastive). If unavailable, code will skip those with a note.\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "    TF_AVAILABLE = True\n",
        "except Exception as e:\n",
        "    TF_AVAILABLE = False\n",
        "\n",
        "# ---------------------- Configuration ----------------------\n",
        "DATASETS = [\n",
        "    \"/mnt/data/Instagram Dataset 4322.csv\",\n",
        "    \"/mnt/data/2022 temple_Mosque_624.csv\",\n",
        "    \"/mnt/data/2021 Cast Full Data_442.csv\",\n",
        "    \"/mnt/data/2021 Indian Politics Full Data_1205.csv\",\n",
        "    \"/mnt/data/2021 Religious Conflicts Full Data_537.csv\",\n",
        "    \"/mnt/data/2022 Hinduphobia_306.csv\",\n",
        "    \"/mnt/data/2022 Historical_hindu_Mu_299.csv\",\n",
        "    \"/mnt/data/2022 Islamophobia_284.csv\",\n",
        "    \"/mnt/data/2022 Namaz_Public_52.csv\",\n",
        "    \"/mnt/data/Youtube comment datset 1295.csv\",\n",
        "]\n",
        "\n",
        "RESULTS_DIR = \"/mnt/data/hate_speech_results_v2\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# If you know exact columns, set them here; otherwise auto-detection will be used.\n",
        "COLUMN_MAP = {\n",
        "    # \"2022 temple_Mosque_624.csv\": {\"text\": \"comment_text\", \"label\": \"label\"},\n",
        "    # \"2021 Cast Full Data_442.csv\": {\"text\": \"text\", \"label\": \"is_hate\"},\n",
        "}\n",
        "\n",
        "# Vectorizer params for ML models\n",
        "TFIDF_PARAMS_ML = dict(\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_features=50000\n",
        ")\n",
        "\n",
        "# Smaller feature space for neural nets to keep memory reasonable\n",
        "TFIDF_PARAMS_NN = dict(\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_features=5000\n",
        ")\n",
        "\n",
        "TEXT_CANDIDATE_KEYWORDS = [\n",
        "    \"text\", \"comment\", \"content\", \"tweet\", \"message\", \"post\", \"body\", \"caption\", \"clean\"\n",
        "]\n",
        "LABEL_CANDIDATE_KEYWORDS = [\n",
        "    \"label\", \"labels\", \"target\", \"class\", \"category\", \"hate\", \"is_hate\", \"toxic\", \"tag\"\n",
        "]\n",
        "\n",
        "# ---------------------- Helpers ----------------------\n",
        "def pick_text_and_label_columns(df: pd.DataFrame):\n",
        "    cols_norm = {c: re.sub(r'[^a-z0-9]+', '_', str(c).strip().lower()) for c in df.columns}\n",
        "    inv = {v: k for k, v in cols_norm.items()}\n",
        "    # text\n",
        "    text_col = None\n",
        "    for key in TEXT_CANDIDATE_KEYWORDS:\n",
        "        for norm, original in inv.items():\n",
        "            if key in norm and df[original].dtype == object:\n",
        "                text_col = original; break\n",
        "        if text_col: break\n",
        "    if text_col is None:\n",
        "        obj_cols = [c for c in df.columns if df[c].dtype == object]\n",
        "        if obj_cols:\n",
        "            text_col = max(obj_cols, key=lambda c: df[c].fillna(\"\").astype(str).str.len().mean())\n",
        "    # label\n",
        "    label_col = None\n",
        "    for key in LABEL_CANDIDATE_KEYWORDS:\n",
        "        for norm, original in inv.items():\n",
        "            if key in norm and original != text_col:\n",
        "                label_col = original; break\n",
        "        if label_col: break\n",
        "    if label_col is None:\n",
        "        candidates = []\n",
        "        for c in df.columns:\n",
        "            if c == text_col: continue\n",
        "            nunq = df[c].nunique(dropna=True)\n",
        "            if nunq <= 6:\n",
        "                candidates.append((c, nunq))\n",
        "        if candidates:\n",
        "            non_obj = [c for c in candidates if df[c[0]].dtype != object]\n",
        "            label_col = (non_obj[0][0] if non_obj else sorted(candidates, key=lambda x: x[1])[0][0])\n",
        "    return text_col, label_col\n",
        "\n",
        "def clean_text(s: pd.Series) -> pd.Series:\n",
        "    s = s.fillna(\"\").astype(str)\n",
        "    return s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "\n",
        "def normalize_binary_labels(y: pd.Series) -> pd.Series:\n",
        "    mapping = {\n",
        "        \"hate\":1, \"hatespeech\":1, \"toxic\":1, \"offensive\":1, \"abusive\":1, \"hs\":1, \"yes\":1, \"y\":1, \"1\":1, 1:1, True:1,\n",
        "        \"non-hate\":0, \"not_hate\":0, \"nonhate\":0, \"clean\":0, \"normal\":0, \"none\":0, \"no\":0, \"n\":0, \"0\":0, 0:0, False:0\n",
        "    }\n",
        "    def m(v):\n",
        "        if pd.isna(v): return np.nan\n",
        "        if v in mapping: return mapping[v]\n",
        "        sv = str(v).strip().lower()\n",
        "        return mapping.get(sv, v)\n",
        "    return y.map(m)\n",
        "\n",
        "def compute_metrics(y_true, y_pred, labels_sorted):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels_sorted)\n",
        "    return acc, prec, rec, f1, cm\n",
        "\n",
        "def save_cm(cm, labels_sorted, path):\n",
        "    cm_df = pd.DataFrame(\n",
        "        cm,\n",
        "        index=[f\"true_{l}\" for l in labels_sorted],\n",
        "        columns=[f\"pred_{l}\" for l in labels_sorted]\n",
        "    )\n",
        "    cm_df.to_csv(path)\n",
        "\n",
        "# ---------------------- Classic ML ----------------------\n",
        "def run_classic_ml(X_text, y, dataset_name):\n",
        "    vec = TfidfVectorizer(**TFIDF_PARAMS_ML)\n",
        "    X_train_txt, X_test_txt, y_train, y_test = train_test_split(\n",
        "        X_text, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y if pd.Series(y).nunique()>1 else None\n",
        "    )\n",
        "    X_train = vec.fit_transform(X_train_txt)\n",
        "    X_test  = vec.transform(X_test_txt)\n",
        "\n",
        "    models = {\n",
        "        \"SVM\": LinearSVC(class_weight=\"balanced\", random_state=RANDOM_SEED),\n",
        "        \"LR\": LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"liblinear\"),\n",
        "        \"NB\": MultinomialNB(),\n",
        "        \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "        \"DT\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_SEED),\n",
        "    }\n",
        "    rows = []\n",
        "    labels_sorted = sorted(pd.unique(y_test))\n",
        "    for name, clf in models.items():\n",
        "        try:\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            acc, prec, rec, f1, cm = compute_metrics(y_test, y_pred, labels_sorted)\n",
        "            cm_path = os.path.join(RESULTS_DIR, f\"{re.sub('[^A-Za-z0-9]+','_',dataset_name)}__{name}__confusion_matrix.csv\")\n",
        "            save_cm(cm, labels_sorted, cm_path)\n",
        "            rows.append({\n",
        "                \"dataset\": dataset_name, \"model\": name, \"kind\": \"classic_ml\",\n",
        "                \"n_train\": len(y_train), \"n_test\": len(y_test),\n",
        "                \"classes\": \";\".join(map(str, labels_sorted)),\n",
        "                \"accuracy\": round(acc,4), \"precision_macro\": round(prec,4),\n",
        "                \"recall_macro\": round(rec,4), \"f1_macro\": round(f1,4),\n",
        "                \"confusion_matrix_path\": cm_path\n",
        "            })\n",
        "        except Exception as e:\n",
        "            rows.append({\n",
        "                \"dataset\": dataset_name, \"model\": name, \"kind\": \"classic_ml\",\n",
        "                \"n_train\": len(y_train), \"n_test\": len(y_test),\n",
        "                \"classes\": \";\".join(map(str, labels_sorted)),\n",
        "                \"accuracy\": np.nan, \"precision_macro\": np.nan, \"recall_macro\": np.nan, \"f1_macro\": np.nan,\n",
        "                \"confusion_matrix_path\": \"\", \"error\": str(e)\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "# ---------------------- CNN/LSTM with TF-IDF-as-sequence ----------------------\n",
        "def build_cnn_model(seq_len, n_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(seq_len, 1)),\n",
        "        layers.Conv1D(64, kernel_size=5, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.Conv1D(64, kernel_size=3, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.GlobalMaxPooling1D(),\n",
        "        layers.Dense(64), layers.ReLU(),\n",
        "        layers.Dense(n_classes, activation=\"softmax\" if n_classes>2 else \"sigmoid\")\n",
        "    ])\n",
        "    loss = \"sparse_categorical_crossentropy\" if n_classes>2 else \"binary_crossentropy\"\n",
        "    model.compile(optimizer=optimizers.Adam(1e-3), loss=loss, metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_lstm_model(seq_len, n_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(seq_len, 1)),\n",
        "        layers.LSTM(64, return_sequences=False),\n",
        "        layers.Dense(64), layers.ReLU(),\n",
        "        layers.Dense(n_classes, activation=\"softmax\" if n_classes>2 else \"sigmoid\")\n",
        "    ])\n",
        "    loss = \"sparse_categorical_crossentropy\" if n_classes>2 else \"binary_crossentropy\"\n",
        "    model.compile(optimizer=optimizers.Adam(1e-3), loss=loss, metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def run_nn_tfidf(X_text, y, dataset_name, arch=\"CNN\", epochs=3, batch_size=64):\n",
        "    if not TF_AVAILABLE:\n",
        "        return [{\n",
        "            \"dataset\": dataset_name, \"model\": arch, \"kind\": \"neural_tfidf\",\n",
        "            \"n_train\": 0, \"n_test\": 0, \"classes\": \"\", \"accuracy\": np.nan,\n",
        "            \"precision_macro\": np.nan, \"recall_macro\": np.nan, \"f1_macro\": np.nan,\n",
        "            \"confusion_matrix_path\": \"\", \"error\": \"TensorFlow not available\"\n",
        "        }]\n",
        "\n",
        "    vec = TfidfVectorizer(**TFIDF_PARAMS_NN)\n",
        "    X_train_txt, X_test_txt, y_train_raw, y_test_raw = train_test_split(\n",
        "        X_text, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y if pd.Series(y).nunique()>1 else None\n",
        "    )\n",
        "    X_train = vec.fit_transform(X_train_txt).astype(np.float32)\n",
        "    X_test  = vec.transform(X_test_txt).astype(np.float32)\n",
        "\n",
        "    # convert to dense and reshape to (seq_len, 1)\n",
        "    X_train = X_train.toarray()\n",
        "    X_test  = X_test.toarray()\n",
        "    seq_len = X_train.shape[1]\n",
        "    X_train = X_train.reshape((-1, seq_len, 1))\n",
        "    X_test  = X_test.reshape((-1, seq_len, 1))\n",
        "\n",
        "    # label encode to 0..C-1 for multiclass;\n",
        "    # for binary we will keep numeric {0,1}\n",
        "    le = LabelEncoder()\n",
        "    y_all = pd.concat([pd.Series(y_train_raw), pd.Series(y_test_raw)], axis=0)\n",
        "    y_all_enc = le.fit_transform(y_all)\n",
        "    n_classes = len(le.classes_)\n",
        "    y_train = y_all_enc[:len(y_train_raw)]\n",
        "    y_test  = y_all_enc[len(y_train_raw):]\n",
        "\n",
        "    es = callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=2, restore_best_weights=True)\n",
        "    if arch.upper() == \"CNN\":\n",
        "        model = build_cnn_model(seq_len, n_classes)\n",
        "    else:\n",
        "        model = build_lstm_model(seq_len, n_classes)\n",
        "    model.fit(\n",
        "        X_train, (y_train if n_classes>2 else y_train.astype(np.float32)),\n",
        "        validation_split=0.1, epochs=epochs, batch_size=batch_size, verbose=0, callbacks=[es]\n",
        "    )\n",
        "    # predictions\n",
        "    probs = model.predict(X_test, verbose=0)\n",
        "    if n_classes > 2:\n",
        "        y_pred_enc = np.argmax(probs, axis=1)\n",
        "    else:\n",
        "        y_pred_enc = (probs.flatten() >= 0.5).astype(int)\n",
        "\n",
        "    # map back to original label values for reporting\n",
        "    y_test_lbl = le.inverse_transform(y_test)\n",
        "    y_pred_lbl = le.inverse_transform(y_pred_enc)\n",
        "\n",
        "    labels_sorted = sorted(pd.unique(y_test_lbl))\n",
        "    acc, prec, rec, f1, cm = compute_metrics(y_test_lbl, y_pred_lbl, labels_sorted)\n",
        "    cm_path = os.path.join(RESULTS_DIR, f\"{re.sub('[^A-Za-z0-9]+','_',dataset_name)}__{arch}__confusion_matrix.csv\")\n",
        "    save_cm(cm, labels_sorted, cm_path)\n",
        "\n",
        "    return [{\n",
        "        \"dataset\": dataset_name, \"model\": arch, \"kind\": \"neural_tfidf\",\n",
        "        \"n_train\": len(y_train), \"n_test\": len(y_test),\n",
        "        \"classes\": \";\".join(map(str, labels_sorted)),\n",
        "        \"accuracy\": round(acc,4), \"precision_macro\": round(prec,4),\n",
        "        \"recall_macro\": round(rec,4), \"f1_macro\": round(f1,4),\n",
        "        \"confusion_matrix_path\": cm_path\n",
        "    }]\n",
        "\n",
        "# ---------------------- Dual Contrastive Learning ----------------------\n",
        "def stochastic_feature_dropout(X_dense, drop_prob=0.1):\n",
        "    mask = (np.random.rand(*X_dense.shape) > drop_prob).astype(np.float32)\n",
        "    return X_dense * mask\n",
        "\n",
        "def supcon_loss_cosine(z1, z2, y, temperature=0.2):\n",
        "    \"\"\"\n",
        "    Supervised contrastive loss across the batch using cosine similarities between z1 and z2 embeddings.\n",
        "    Combines both views. y are integer class labels.\n",
        "    \"\"\"\n",
        "    # normalize\n",
        "    z1 = z1 / (np.linalg.norm(z1, axis=1, keepdims=True) + 1e-8)\n",
        "    z2 = z2 / (np.linalg.norm(z2, axis=1, keepdims=True) + 1e-8)\n",
        "    Z = np.concatenate([z1, z2], axis=0)  # [2B, d]\n",
        "    Y = np.concatenate([y, y], axis=0)    # [2B]\n",
        "    B = Z.shape[0]\n",
        "\n",
        "    # cosine sim matrix\n",
        "    sim = Z @ Z.T  # [2B, 2B]\n",
        "    # remove self\n",
        "    np.fill_diagonal(sim, -1e9)\n",
        "    # scale\n",
        "    sim /= temperature\n",
        "\n",
        "    # mask for positives per anchor\n",
        "    pos_mask = (Y.reshape(-1,1) == Y.reshape(1,-1)).astype(np.float32)\n",
        "    np.fill_diagonal(pos_mask, 0.0)\n",
        "\n",
        "    # log-softmax over rows\n",
        "    sim_max = sim.max(axis=1, keepdims=True)\n",
        "    exp_sim = np.exp(sim - sim_max)\n",
        "    denom = exp_sim.sum(axis=1, keepdims=True)\n",
        "    log_prob = sim - sim_max - np.log(denom + 1e-12)\n",
        "\n",
        "    # average over positive pairs\n",
        "    pos_counts = pos_mask.sum(axis=1, keepdims=True) + 1e-8\n",
        "    loss_per_anchor = -(log_prob * pos_mask).sum(axis=1, keepdims=True) / pos_counts\n",
        "    return loss_per_anchor.mean()\n",
        "\n",
        "def run_dual_contrastive(X_text, y, dataset_name, epochs=10, batch_size=128, proj_dim=128, lr=1e-3, drop_prob=0.1):\n",
        "    \"\"\"\n",
        "    Two TF-IDF views via stochastic feature dropout.\n",
        "    Train a small projection head with supervised contrastive loss.\n",
        "    Then train LR on frozen embeddings and evaluate.\n",
        "    \"\"\"\n",
        "    if not TF_AVAILABLE:\n",
        "        return [{\n",
        "            \"dataset\": dataset_name, \"model\": \"DualContrastive\", \"kind\": \"contrastive\",\n",
        "            \"n_train\": 0, \"n_test\": 0, \"classes\": \"\", \"accuracy\": np.nan,\n",
        "            \"precision_macro\": np.nan, \"recall_macro\": np.nan, \"f1_macro\": np.nan,\n",
        "            \"confusion_matrix_path\": \"\", \"error\": \"TensorFlow not available\"\n",
        "        }]\n",
        "\n",
        "    vec = TfidfVectorizer(**TFIDF_PARAMS_NN)\n",
        "    X_train_txt, X_test_txt, y_train, y_test = train_test_split(\n",
        "        X_text, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y if pd.Series(y).nunique()>1 else None\n",
        "    )\n",
        "    X_train = vec.fit_transform(X_train_txt).astype(np.float32).toarray()\n",
        "    X_test  = vec.transform(X_test_txt).astype(np.float32).toarray()\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_train_enc = le.fit_transform(y_train)\n",
        "    y_test_enc  = le.transform(y_test)\n",
        "    n_classes = len(le.classes_)\n",
        "\n",
        "    # Projection head (simple MLP in TF)\n",
        "    d_in = X_train.shape[1]\n",
        "    inp = layers.Input(shape=(d_in,))\n",
        "    x = layers.Dense(256, activation=\"relu\")(inp)\n",
        "    x = layers.Dense(proj_dim, activation=None)(x)\n",
        "    proj_model = models.Model(inp, x)\n",
        "    opt = optimizers.Adam(lr)\n",
        "\n",
        "    # Manual training loop for SupCon loss using NumPy for loss, TF for forward\n",
        "    # to keep code compact and dependency-light\n",
        "    @tf.function\n",
        "    def forward(batch):\n",
        "        return proj_model(batch, training=True)\n",
        "\n",
        "    train_idx = np.arange(X_train.shape[0])\n",
        "    steps = math.ceil(len(train_idx)/batch_size)\n",
        "    for epoch in range(epochs):\n",
        "        np.random.shuffle(train_idx)\n",
        "        epoch_loss = []\n",
        "        for step in range(steps):\n",
        "            sl = step*batch_size\n",
        "            sr = min((step+1)*batch_size, len(train_idx))\n",
        "            idx = train_idx[sl:sr]\n",
        "            xb = X_train[idx]\n",
        "            yb = y_train_enc[idx]\n",
        "\n",
        "            # two stochastic views\n",
        "            v1 = stochastic_feature_dropout(xb, drop_prob=drop_prob)\n",
        "            v2 = stochastic_feature_dropout(xb, drop_prob=drop_prob)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                z1 = forward(tf.convert_to_tensor(v1))\n",
        "                z2 = forward(tf.convert_to_tensor(v2))\n",
        "                # compute SupCon loss in numpy for simplicity\n",
        "                z1_np = z1.numpy()\n",
        "                z2_np = z2.numpy()\n",
        "                loss_val = supcon_loss_cosine(z1_np, z2_np, yb, temperature=0.2)\n",
        "                loss_tf = tf.convert_to_tensor(loss_val, dtype=tf.float32)\n",
        "\n",
        "            grads = tape.gradient(loss_tf, proj_model.trainable_variables)\n",
        "            opt.apply_gradients(zip(grads, proj_model.trainable_variables))\n",
        "            epoch_loss.append(loss_val)\n",
        "\n",
        "        # print(f\"[DualCon] {dataset_name} epoch {epoch+1}/{epochs} loss={np.mean(epoch_loss):.4f}\")\n",
        "\n",
        "    # Freeze projection, compute embeddings\n",
        "    Z_train = proj_model.predict(X_train, verbose=0)\n",
        "    Z_test  = proj_model.predict(X_test, verbose=0)\n",
        "\n",
        "    # Train LR head on embeddings\n",
        "    head = LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"liblinear\")\n",
        "    head.fit(Z_train, y_train_enc)\n",
        "    y_pred_enc = head.predict(Z_test)\n",
        "    y_pred = le.inverse_transform(y_pred_enc)\n",
        "\n",
        "    labels_sorted = sorted(pd.unique(y_test))\n",
        "    acc, prec, rec, f1, cm = compute_metrics(y_test, y_pred, labels_sorted)\n",
        "    cm_path = os.path.join(RESULTS_DIR, f\"{re.sub('[^A-Za-z0-9]+','_',dataset_name)}__DualContrastive__confusion_matrix.csv\")\n",
        "    save_cm(cm, labels_sorted, cm_path)\n",
        "\n",
        "    return [{\n",
        "        \"dataset\": dataset_name, \"model\": \"DualContrastive\", \"kind\": \"contrastive\",\n",
        "        \"n_train\": len(y_train), \"n_test\": len(y_test),\n",
        "        \"classes\": \";\".join(map(str, labels_sorted)),\n",
        "        \"accuracy\": round(acc,4), \"precision_macro\": round(prec,4),\n",
        "        \"recall_macro\": round(rec,4), \"f1_macro\": round(f1,4),\n",
        "        \"confusion_matrix_path\": cm_path\n",
        "    }]\n",
        "\n",
        "# ---------------------- Main loop ----------------------\n",
        "def process_dataset(path):\n",
        "    name = os.path.basename(path)\n",
        "    if not os.path.exists(path):\n",
        "        return [], {\"dataset\": name, \"reason\": \"file not found\"}\n",
        "\n",
        "    # read CSV\n",
        "    try:\n",
        "        try: df = pd.read_csv(path)\n",
        "        except Exception: df = pd.read_csv(path, encoding=\"latin-1\")\n",
        "    except Exception as e:\n",
        "        return [], {\"dataset\": name, \"reason\": f\"read error: {e}\"}\n",
        "\n",
        "    df = df.dropna(axis=1, how=\"all\")\n",
        "    if df.empty or df.shape[1] == 0:\n",
        "        return [], {\"dataset\": name, \"reason\": \"empty file\"}\n",
        "\n",
        "    # column selection\n",
        "    if name in COLUMN_MAP:\n",
        "        text_col = COLUMN_MAP[name][\"text\"]\n",
        "        label_col = COLUMN_MAP[name][\"label\"]\n",
        "        if text_col not in df.columns or label_col not in df.columns:\n",
        "            tcols = list(df.columns)\n",
        "            return [], {\"dataset\": name, \"reason\": f\"column mapping not found in file\", \"columns\": tcols}\n",
        "    else:\n",
        "        text_col, label_col = pick_text_and_label_columns(df)\n",
        "    if text_col is None or label_col is None:\n",
        "        return [], {\"dataset\": name, \"reason\": f\"could not detect columns (text={text_col}, label={label_col})\",\n",
        "                    \"columns\": list(df.columns)}\n",
        "\n",
        "    X_text = clean_text(df[text_col])\n",
        "    y = normalize_binary_labels(df[label_col])\n",
        "\n",
        "    mask = (X_text.str.len() > 0) & (~pd.Series(y).isna())\n",
        "    X_text = X_text[mask]\n",
        "    y = pd.Series(y)[mask]\n",
        "\n",
        "    if pd.Series(y).nunique() < 2:\n",
        "        return [], {\"dataset\": name, \"reason\": \"only one class after cleaning\", \"label_col\": label_col}\n",
        "\n",
        "    results = []\n",
        "    # Classic ML\n",
        "    results += run_classic_ml(X_text, y, name)\n",
        "    # CNN & LSTM\n",
        "    results += run_nn_tfidf(X_text, y, name, arch=\"CNN\", epochs=3, batch_size=64)\n",
        "    results += run_nn_tfidf(X_text, y, name, arch=\"LSTM\", epochs=3, batch_size=64)\n",
        "    # Dual Contrastive\n",
        "    results += run_dual_contrastive(X_text, y, name, epochs=10, batch_size=128, proj_dim=128, lr=1e-3, drop_prob=0.1)\n",
        "\n",
        "    return results, None\n",
        "\n",
        "def main():\n",
        "    all_rows = []\n",
        "    skipped = []\n",
        "    for p in DATASETS:\n",
        "        rows, skip = process_dataset(p)\n",
        "        if rows:\n",
        "            all_rows.extend(rows)\n",
        "        if skip:\n",
        "            skipped.append(skip)\n",
        "    # save summary files\n",
        "    summary = pd.DataFrame(all_rows)\n",
        "    summary_path = os.path.join(RESULTS_DIR, \"summary_metrics.csv\")\n",
        "    summary.to_csv(summary_path, index=False)\n",
        "\n",
        "    skipped_df = pd.DataFrame(skipped)\n",
        "    skipped_path = os.path.join(RESULTS_DIR, \"skipped_datasets.csv\")\n",
        "    skipped_df.to_csv(skipped_path, index=False)\n",
        "\n",
        "    print(\"Saved:\", summary_path)\n",
        "    print(\"Saved:\", skipped_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}